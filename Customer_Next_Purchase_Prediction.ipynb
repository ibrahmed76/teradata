{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab150047",
   "metadata": {},
   "source": [
    "# ***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5529dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from teradataml import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn2pmml import sklearn2pmml, PMMLPipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76026b2",
   "metadata": {},
   "source": [
    "# ***Connect to Teradata Vantage***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '172.16.0.12'\n",
    "username = getpass.getpass(\"Username: \")\n",
    "password = getpass.getpass(\"Password: \")\n",
    "\n",
    "# Create connection context\n",
    "eng = create_context(host=host, username=username, password=password)\n",
    "print(\"Connected to Vantage:\", eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac1fe7",
   "metadata": {},
   "source": [
    "# ***Data exploration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db4b58",
   "metadata": {},
   "source": [
    "### Load data from Vantage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame(in_schema('Source_data_db', 'jcr_fake_events'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head().show_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a3f25",
   "metadata": {},
   "source": [
    "### Check data types and basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff692c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\", tdf.shape)\n",
    "print(\"\\nColumn information:\")\n",
    "tdf.info()\n",
    "print(\"\\nBasic statistics:\")\n",
    "tdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf74226",
   "metadata": {},
   "source": [
    "### Load and preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Teradata DataFrame\n",
    "df1 = DataFrame(in_schema(\"source_data_db\", \"jcr_fake_events\"))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas\n",
    "df1 = df1.to_pandas()\n",
    "\n",
    "# Check column names\n",
    "print(\"Columns:\", df1.columns)\n",
    "\n",
    "# Normalize just in case\n",
    "df1.columns = df1.columns.str.strip().str.lower()\n",
    "print(\"Normalized columns:\", df1.columns)\n",
    "\n",
    "# Proceed with timestamp conversion and plotting\n",
    "df1['ts'] = pd.to_datetime(df1['ts'])\n",
    "\n",
    "# Now 'customer_id' should be present\n",
    "print(df1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f65387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df1.columns = df1.columns.str.strip().str.lower()\n",
    "\n",
    "# Convert timestamp column\n",
    "df1['ts'] = pd.to_datetime(df1['ts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Product purchase frequency\n",
    "product_counts = df1['product_id'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(product_counts.index, product_counts.values, color='skyblue')\n",
    "plt.title('Product Purchase Frequency')\n",
    "plt.xlabel('Product ID')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Customer purchase distribution\n",
    "customer_purchases = df1['customer_id'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(customer_purchases.values, bins=20, color='lightgreen', alpha=0.7)\n",
    "plt.title('Purchases per Customer')\n",
    "plt.xlabel('Number of Purchases')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Price distribution by product\n",
    "plt.figure(figsize=(10, 6))\n",
    "df1.boxplot(column='price', by='product_id')\n",
    "plt.title('Price Distribution by Product')\n",
    "plt.suptitle('')  # remove default title\n",
    "plt.xlabel('Product ID')\n",
    "plt.ylabel('Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Purchases over time\n",
    "df_time = df1.set_index('ts').resample('D').size()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_time.index, df_time.values, color='coral')\n",
    "plt.title('Purchases Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651eeb42",
   "metadata": {},
   "source": [
    "# ***Analyze the sequence of purchase of products by a customer***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e3bc2",
   "metadata": {},
   "source": [
    "# Copy to Vantage for nPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df1[['customer_id', 'product_id', 'price' ,'ts']], table_name='npath_purchase_data', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a66371",
   "metadata": {},
   "source": [
    "# Run NPath to detect sequential purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "npath_result = NPath(\n",
    "    data1=DataFrame(\"npath_purchase_data\"),\n",
    "    data1_partition_column=[\"customer_id\"],\n",
    "    data1_order_column=\"ts\",\n",
    "    mode=\"NONOVERLAPPING\",\n",
    "    symbols=[\"TRUE AS A\", \"TRUE AS B\"],\n",
    "    pattern=\"A.B\",\n",
    "    result=[\n",
    "        \"FIRST(product_id OF A) AS current_product\",\n",
    "        \"FIRST(product_id OF B) AS next_product\",\n",
    "        \"FIRST(customer_id OF A) AS customer_id\",\n",
    "        \"FIRST(ts OF A) AS current_ts\",\n",
    "        \"FIRST(ts OF B) AS next_ts\",\n",
    "        \"FIRST(price OF A) AS current_price\",\n",
    "        \"FIRST(price OF B) AS next_price\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "purchase_pairs_df = npath_result.result.to_pandas()\n",
    "purchase_pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f7384",
   "metadata": {},
   "source": [
    "# ***Generate target variable***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f23db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_analysis = (\n",
    "    purchase_pairs_df.groupby([\"current_product\", \"next_product\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .sort_values(by=\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "transition_analysis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed6f2c",
   "metadata": {},
   "source": [
    "# Time-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_df = purchase_pairs_df.copy()\n",
    "sequences_df[\"current_ts\"] = pd.to_datetime(sequences_df[\"current_ts\"])\n",
    "sequences_df[\"next_ts\"] = pd.to_datetime(sequences_df[\"next_ts\"])\n",
    "\n",
    "sequences_df[\"time_between_purchases\"] = (sequences_df[\"next_ts\"] - sequences_df[\"current_ts\"]).dt.total_seconds() / 3600  # in hours\n",
    "sequences_df[\"hour\"] = sequences_df[\"current_ts\"].dt.hour\n",
    "sequences_df[\"day_of_week\"] = sequences_df[\"current_ts\"].dt.dayofweek\n",
    "sequences_df[\"day_of_month\"] = sequences_df[\"current_ts\"].dt.day\n",
    "sequences_df[\"is_weekend\"] = sequences_df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "sequences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb3fb",
   "metadata": {},
   "source": [
    "# Customer-level aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"ts\"] = pd.to_datetime(df1[\"ts\"])\n",
    "\n",
    "customer_features = (\n",
    "    df1.groupby(\"customer_id\")\n",
    "    .agg(\n",
    "        total_purchases=(\"product_id\", \"count\"),\n",
    "        avg_price=(\"price\", \"mean\"),\n",
    "        max_price=(\"price\", \"max\"),\n",
    "        min_price=(\"price\", \"min\"),\n",
    "        first_purchase=(\"ts\", \"min\"),\n",
    "        last_purchase=(\"ts\", \"max\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "customer_features[\"purchase_frequency\"] = (\n",
    "    (customer_features[\"last_purchase\"] - customer_features[\"first_purchase\"]).dt.days\n",
    "    / customer_features[\"total_purchases\"].replace(0, 1)\n",
    ")\n",
    "customer_features[\"customer_lifetime_days\"] = (\n",
    "    customer_features[\"last_purchase\"] - customer_features[\"first_purchase\"]\n",
    ").dt.days\n",
    "\n",
    "reference_date = df1[\"ts\"].max()\n",
    "customer_features[\"recency_days\"] = (reference_date - customer_features[\"last_purchase\"]).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437014b7",
   "metadata": {},
   "source": [
    "# Final Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sequence data with customer features to create final modeling dataset\n",
    "print(\"Creating final modeling dataset...\")\n",
    "model_data = sequences_df.merge(customer_features, on='customer_id', how='left')\n",
    "\n",
    "print(f\"Initial merged dataset shape: {model_data.shape}\")\n",
    "print(f\"Available columns: {list(model_data.columns)}\")\n",
    "\n",
    "# Create temporal features if they don't exist\n",
    "if 'current_ts' in model_data.columns:\n",
    "    model_data['current_ts'] = pd.to_datetime(model_data['current_ts'])\n",
    "    \n",
    "    # Create missing temporal features\n",
    "    if 'current_hour' not in model_data.columns:\n",
    "        model_data['current_hour'] = model_data['current_ts'].dt.hour\n",
    "        print(\"✓ Created current_hour feature\")\n",
    "    \n",
    "    if 'current_day_of_week' not in model_data.columns:\n",
    "        model_data['current_day_of_week'] = model_data['current_ts'].dt.dayofweek\n",
    "        print(\"✓ Created current_day_of_week feature\")\n",
    "else:\n",
    "    print(\"WARNING: No current_ts column found, using default temporal features\")\n",
    "    model_data['current_hour'] = 12  # Default noon\n",
    "    model_data['current_day_of_week'] = 2  # Default Wednesday\n",
    "\n",
    "# Create robust engineered features with safety checks\n",
    "model_data['price_ratio'] = model_data['current_price'] / (model_data['avg_price'] + 0.01)  # Avoid division by zero\n",
    "model_data['is_expensive_product'] = (model_data['current_price'] > model_data['avg_price']).astype(int)\n",
    "\n",
    "# Create is_weekend feature (now we're sure current_day_of_week exists)\n",
    "model_data['is_weekend'] = model_data['current_day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Handle missing values safely\n",
    "numeric_columns = ['std_price', 'customer_lifetime_days', 'purchase_frequency']\n",
    "for col in numeric_columns:\n",
    "    if col in model_data.columns:\n",
    "        model_data[col] = model_data[col].fillna(0)\n",
    "\n",
    "# Remove rows with missing target\n",
    "initial_rows = len(model_data)\n",
    "model_data = model_data.dropna(subset=['next_product'])\n",
    "final_rows = len(model_data)\n",
    "\n",
    "print(f\"Data cleaned: {initial_rows} -> {final_rows} rows\")\n",
    "print(f\"Target variable distribution:\")\n",
    "if final_rows > 0:\n",
    "    print(model_data['next_product'].value_counts())\n",
    "    print(f\"Available columns after processing: {list(model_data.columns)}\")\n",
    "else:\n",
    "    print(\"ERROR: No valid data for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e91881",
   "metadata": {},
   "source": [
    "# ***Generate a predictive model using lightGBM and sklearn***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b7c30",
   "metadata": {},
   "source": [
    "# Model Training with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89997e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have sufficient data for modeling\n",
    "if 'model_data' not in locals() or len(model_data) < 10:\n",
    "    print(\"ERROR: Insufficient data for modeling. Need at least 10 samples.\")\n",
    "    print(\"This likely means the NPath analysis didn't find enough sequential patterns.\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"1. Checking if customers have multiple purchases\")\n",
    "    print(\"2. Verifying the date range has sufficient data\")\n",
    "    print(\"3. Reducing the time window for sequence detection\")\n",
    "else:\n",
    "    print(f\"Proceeding with modeling using {len(model_data)} samples\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_current = LabelEncoder()\n",
    "    le_target = LabelEncoder()\n",
    "    \n",
    "    model_data['current_product_encoded'] = le_current.fit_transform(model_data['current_product'])\n",
    "    model_data['next_product_encoded'] = le_target.fit_transform(model_data['next_product'])\n",
    "    \n",
    "    # Define available features (only use columns that exist)\n",
    "    potential_features = [\n",
    "        'current_price', 'time_between_purchases', 'current_hour', 'current_day_of_week',\n",
    "        'avg_price', 'std_price', 'min_price', 'max_price', 'total_purchases',\n",
    "        'customer_lifetime_days', 'purchase_frequency', 'price_ratio', \n",
    "        'is_expensive_product', 'is_weekend', 'current_product_encoded'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    available_features = [f for f in potential_features if f in model_data.columns]\n",
    "    print(f\"Using features: {available_features}\")\n",
    "    \n",
    "    # Prepare final datasets\n",
    "    X = model_data[available_features].copy()\n",
    "    y = model_data['next_product_encoded'].copy()\n",
    "    \n",
    "    # Check for any remaining data issues\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target classes: {len(le_target.classes_)} products: {list(le_target.classes_)}\")\n",
    "    \n",
    "    # Verify no infinite or missing values\n",
    "    infinite_cols = X.columns[X.isin([np.inf, -np.inf]).any()].tolist()\n",
    "    if infinite_cols:\n",
    "        print(f\"WARNING: Infinite values found in: {infinite_cols}\")\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    missing_cols = X.columns[X.isna().any()].tolist()\n",
    "    if missing_cols:\n",
    "        print(f\"WARNING: Missing values found in: {missing_cols}\")\n",
    "        X = X.fillna(0)\n",
    "    \n",
    "    # Split data for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split completed:\")\n",
    "    print(f\"• Training set: {X_train.shape}\")\n",
    "    print(f\"• Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Configure LightGBM with more conservative settings for small datasets\n",
    "    \n",
    "    n_samples = len(X_train)\n",
    "    lgbm_model = lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=len(le_target.classes_),\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=min(31, max(2, n_samples // 10)),  # Adaptive num_leaves\n",
    "        learning_rate=0.1,  # Higher learning rate for small datasets\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        min_data_in_leaf=max(1, n_samples // 50),  # Adaptive min_data_in_leaf\n",
    "        verbose=-1,  # Suppress warnings\n",
    "        random_state=42,\n",
    "        n_estimators=min(100, max(10, n_samples // 2)),  # Adaptive n_estimators\n",
    "        force_col_wise=True  # For better performance on small datasets\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining LightGBM model with {lgbm_model.n_estimators} estimators...\")\n",
    "    print(f\"Model configured for dataset size: {n_samples} samples\")\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    y_pred_proba = lgbm_model.predict_proba(X_test)\n",
    "    \n",
    "    print(\"✓ Model training completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962d704",
   "metadata": {},
   "source": [
    "# Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f332362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance (only if model was trained successfully)\n",
    "if 'model_data' in locals() and len(model_data) >= 10 and 'lgbm_model' in locals():\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Model Performance Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"• Overall Accuracy: {accuracy:.4f} ({accuracy:.1%})\")\n",
    "    \n",
    "    # Show detailed classification report\n",
    "    if len(le_target.classes_) <= 10:  # Only show detailed report for manageable number of classes\n",
    "        print(f\"\\nDetailed Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': lgbm_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    print(feature_importance.head(5).to_string(index=False))\n",
    "    \n",
    "    # Create a simple visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(min(10, len(feature_importance)))\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top Feature Importance for Next Purchase Prediction')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Model evaluation skipped - no trained model available\")\n",
    "    print(\"This indicates insufficient sequential purchase data in your dataset\")\n",
    "    if 'model_data' not in locals():\n",
    "        print(\"ERROR: model_data variable not found - please run previous cells first\")\n",
    "    elif len(model_data) < 10:\n",
    "        print(f\"ERROR: Only {len(model_data)} samples available, need at least 10\")\n",
    "    else:\n",
    "        print(\"ERROR: lgbm_model not found - model training may have failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea3b2e",
   "metadata": {},
   "source": [
    "# ***Import model in Vantage***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cbdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained LightGBM model to PMML and import into Vantage (BYOM)\n",
    "if 'lgbm_model' in locals() and 'X_train' in locals() and 'y_train' in locals():\n",
    "    try:\n",
    "        # Wrap the model in a PMMLPipeline\n",
    "        pipeline = PMMLPipeline([(\"classifier\", lgbm_model)])\n",
    "        pipeline.fit(X_train, y_train)  # Fit again to ensure pipeline is ready\n",
    "        pmml_filename = \"next_purchase_lgbm.pmml\"\n",
    "        sklearn2pmml(pipeline, pmml_filename)\n",
    "        print(f\"Model exported to {pmml_filename}\")\n",
    "        \n",
    "        byom_table = \"byom_models\"  # Table to store BYOM models\n",
    "        model_id = \"next_purchase_lgbm\"\n",
    "        save_byom(model_id=model_id, model_file=pmml_filename, table_name=byom_table)\n",
    "        print(f\"Model {model_id} imported into Vantage in table {byom_table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during BYOM export/import: {e}\")\n",
    "else:\n",
    "    print(\"Model not trained or missing data. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd84f2c",
   "metadata": {},
   "source": [
    "# ***Score the model in-database***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b41f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model in-database using PMMLPredict\n",
    "try:\n",
    "    # Retrieve the model from Vantage\n",
    "    model_tdf = retrieve_byom(\"next_purchase_lgbm\", table_name=\"byom_models\")\n",
    "    # Prepare test data from Vantage (adjust table/columns as needed)\n",
    "    test_data = DataFrame(\"npath_purchase_data\")\n",
    "    # Score the model in-database\n",
    "    scored_results = PMMLPredict(\n",
    "        modeldata=model_tdf,\n",
    "        newdata=test_data,\n",
    "        accumulate=['customer_id', 'product_id', 'price', 'ts'],  # Adjust as needed\n",
    "        overwrite_cached_models='*'\n",
    "    )\n",
    "    print(\"In-database scoring completed. Sample results:\")\n",
    "    display(scored_results.result.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error during in-database scoring: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
