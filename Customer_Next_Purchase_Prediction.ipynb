{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab150047",
   "metadata": {},
   "source": [
    "# ***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5529dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from teradataml import *\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn2pmml import sklearn2pmml, PMMLPipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from tdnpathviz.visualizations import plot_first_main_paths\n",
    "\n",
    "# Configure BYOM and VAL install locations (crucial for BYOM to work)\n",
    "configure.byom_install_location = 'mldb'\n",
    "configure.val_install_location = 'val'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76026b2",
   "metadata": {},
   "source": [
    "# ***Connect to Teradata Vantage***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '172.16.0.12'\n",
    "username = getpass.getpass(\"Username: \")\n",
    "password = getpass.getpass(\"Password: \")\n",
    "\n",
    "# Create connection context\n",
    "eng = create_context(host=host, username=username, password=password)\n",
    "print(\"Connected to Vantage:\", eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac1fe7",
   "metadata": {},
   "source": [
    "# ***Data exploration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db4b58",
   "metadata": {},
   "source": [
    "### Load data from Vantage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame(in_schema('Source_data_db', 'jcr_fake_events'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head().show_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a3f25",
   "metadata": {},
   "source": [
    "### Check data types and basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff692c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\", tdf.shape)\n",
    "print(\"\\nColumn information:\")\n",
    "tdf.info()\n",
    "print(\"\\nBasic statistics:\")\n",
    "tdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf74226",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group by product and count transactions\n",
    "transactions_by_product = tdf.groupby('product_id').count().get(['product_id', 'count_customer_id'])\n",
    "\n",
    "# 2. Sort by count in descending order\n",
    "transactions_by_product = transactions_by_product.sort('count_customer_id', ascending=False)\n",
    "transactions_by_product = transactions_by_product.assign(\n",
    "    product_int = case([\n",
    "        (transactions_by_product.product_id == \"Almonds\", 0),\n",
    "        (transactions_by_product.product_id == \"Blanket\", 1),\n",
    "        (transactions_by_product.product_id == \"Blender\", 2),\n",
    "        (transactions_by_product.product_id == \"Charger\", 3),\n",
    "        (transactions_by_product.product_id == \"Chocolate\", 4),\n",
    "        (transactions_by_product.product_id == \"Cleanser\", 5),\n",
    "        (transactions_by_product.product_id == \"Coffee\", 6),\n",
    "        (transactions_by_product.product_id == \"Cookies\", 7),\n",
    "        (transactions_by_product.product_id == \"Cookware\", 8),\n",
    "        (transactions_by_product.product_id == \"Cutlery\", 9),\n",
    "        (transactions_by_product.product_id == \"Diffuser\", 10),\n",
    "        (transactions_by_product.product_id == \"Granola\", 11),\n",
    "        (transactions_by_product.product_id == \"Hairdryer\", 12),\n",
    "        (transactions_by_product.product_id == \"Headset\", 13),\n",
    "        (transactions_by_product.product_id == \"Honey\", 14),\n",
    "        (transactions_by_product.product_id == \"Humidifier\", 15),\n",
    "        (transactions_by_product.product_id == \"Kettle\", 16),\n",
    "        (transactions_by_product.product_id == \"Keyboard\", 17),\n",
    "        (transactions_by_product.product_id == \"Lotion\", 18),\n",
    "        (transactions_by_product.product_id == \"Moisturizer\", 19),\n",
    "        (transactions_by_product.product_id == \"Mouse\", 20),\n",
    "        (transactions_by_product.product_id == \"Oliveoil\", 21),\n",
    "        (transactions_by_product.product_id == \"Organizer\", 22),\n",
    "        (transactions_by_product.product_id == \"Perfume\", 23),\n",
    "        (transactions_by_product.product_id == \"Pillow\", 24),\n",
    "        (transactions_by_product.product_id == \"Popcorn\", 25),\n",
    "        (transactions_by_product.product_id == \"Router\", 26),\n",
    "        (transactions_by_product.product_id == \"Serum\", 27),\n",
    "        (transactions_by_product.product_id == \"Shampoo\", 28),\n",
    "        (transactions_by_product.product_id == \"Skincare\", 29),\n",
    "        (transactions_by_product.product_id == \"Speaker\", 30),\n",
    "        (transactions_by_product.product_id == \"Sunscreen\", 31),\n",
    "        (transactions_by_product.product_id == \"Tea\", 32),\n",
    "        (transactions_by_product.product_id == \"Toaster\", 33),\n",
    "        (transactions_by_product.product_id == \"Toothbrush\", 34),\n",
    "        (transactions_by_product.product_id == \"Truffles\", 35),\n",
    "        (transactions_by_product.product_id == \"Webcam\", 36),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e708412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_by_product = tdf.groupby('product_id').count().get(['product_id', 'count_customer_id'])\n",
    "# transactions_by_product = transactions_by_product.sort('count_customer_id', ascending=False)\n",
    "# # Example alternative: map product categories to their index dynamically\n",
    "# product_categories = [\n",
    "#     \"Almonds\", \"Blanket\", \"Blender\", \"Charger\", \"Chocolate\", \"Cleanser\", \"Coffee\", \n",
    "#     \"Cookies\", \"Cookware\", \"Cutlery\", \"Diffuser\", \"Granola\", \"Hairdryer\", \"Headset\", \n",
    "#     \"Honey\", \"Humidifier\", \"Kettle\", \"Keyboard\", \"Lotion\", \"Moisturizer\", \"Mouse\",\n",
    "#     \"Oliveoil\", \"Organizer\", \"Perfume\", \"Pillow\", \"Popcorn\", \"Router\", \"Serum\", \n",
    "#     \"Shampoo\", \"Skincare\", \"Speaker\", \"Sunscreen\", \"Tea\", \"Toaster\", \"Toothbrush\", \n",
    "#     \"Truffles\", \"Webcam\"\n",
    "# ]\n",
    "# mapping_expr = case([\n",
    "#     (transactions_by_product.product_id == prod, idx) \n",
    "#     for idx, prod in enumerate(product_categories)\n",
    "# ])\n",
    "# transactions_by_product = transactions_by_product.assign(product_int = mapping_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Product Purchase Frequency - Bar Plot (By Product index)\n",
    "transactions_by_product.plot(\n",
    "    x=transactions_by_product.product_int,\n",
    "    y=transactions_by_product.count_customer_id,\n",
    "    kind='bar',\n",
    "    title=\"Number of Transactions per Product\",\n",
    "    ylabel='Count of Transactions',\n",
    "    xlabel='Product (indexed)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "pdf = transactions_by_product.to_pandas(all_rows=True)\n",
    "\n",
    "# Use Teradata .plot() but with custom parameters for better appearance\n",
    "pdf.plot(\n",
    "    x='product_id',\n",
    "    y='count_customer_id',\n",
    "    kind='bar',\n",
    "    color='skyblue',   # Bar color\n",
    "    legend=False,\n",
    "    figsize=(10, 5),   # Similar to plt.figure(figsize=(10,5))\n",
    "    title=\"Product Purchase Frequency\",\n",
    "    xlabel='Product ID',\n",
    "    ylabel='Number of Purchases'\n",
    ").tick_params(axis='x', rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated data for product frequency\n",
    "product_freq = df1.groupby(['product_id']).count()\n",
    "product_freq = product_freq.assign(purchase_count=product_freq.count_customer_id)\n",
    "product_freq = product_freq.select(['product_id', 'purchase_count'])\n",
    "product_freq = product_freq.sort(['purchase_count'], ascending=False)\n",
    "\n",
    "# Filter top 10 products for better visualization\n",
    "df2 = product_freq.head(10)\n",
    "df2.plot(x=df2.product_id, \n",
    "        y=df2.purchase_count, \n",
    "        kind=\"bar\",\n",
    "        title=\"Top 10 Products by Purchase Frequency\",\n",
    "        color=\"skyblue\",\n",
    "        xlabel=\"Product ID\",\n",
    "        ylabel=\"Purchase Count\",\n",
    "        grid_linestyle=\"-\",\n",
    "        grid_linewidth=0.5 \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Line PLot\n",
    "pdf = tdf.to_pandas(all_rows=True)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "pdf['ts'] = pd.to_datetime(pdf['ts'])\n",
    "\n",
    "# Aggregate purchases per day\n",
    "daily_counts = pdf.groupby(pdf['ts'].dt.date).size().reset_index(name='purchase_count')\n",
    "\n",
    "# Plot using pandas built-in plotting (similar to df1.plot)\n",
    "plot = daily_counts.plot(\n",
    "    x='ts', \n",
    "    y='purchase_count', \n",
    "    kind='line',\n",
    "    title=\"Purchases Over Time\",\n",
    "    legend=False,\n",
    "    xlabel=\"Date\",\n",
    "    ylabel=\"Number of Purchases\",\n",
    "    figsize=(12, 6)\n",
    "    \n",
    ").tick_params(axis='x', rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651eeb42",
   "metadata": {},
   "source": [
    "# ***Analyze the sequence of purchase of products by a customer***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e3bc2",
   "metadata": {},
   "source": [
    "# Copy to Vantage for nPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(tdf[['customer_id', 'product_id', 'price' ,'ts']], table_name='npath_purchase_data', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a66371",
   "metadata": {},
   "source": [
    "# Run NPath to detect sequential purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "npath_result = NPath(\n",
    "    data1=DataFrame(\"npath_purchase_data\"),\n",
    "    data1_partition_column=[\"customer_id\"],\n",
    "    data1_order_column=[\"customer_id\",\"ts\"],\n",
    "    mode=\"NONOVERLAPPING\",\n",
    "    symbols=[\"TRUE AS A\", \"TRUE AS B\"],\n",
    "    pattern=\"(A){4}.B\",\n",
    "    result=[\n",
    "        \"ACCUMULATE(product_id OF A) AS current_products\",\n",
    "        \"FIRST(customer_id OF A) AS customer_id\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "purchase_pairs_df = npath_result.result\n",
    "purchase_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b28427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_main_paths(purchase_pairs_df, path_column = 'current_products', id_column = 'customer_id',width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f7384",
   "metadata": {},
   "source": [
    "# ***Generate target variable***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133dad9",
   "metadata": {},
   "source": [
    "## Create a feature table that includes RFM-based features (recency, frequency, and monetary values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f23db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(tdf[['customer_id', 'product_id', 'price' ,'ts']], table_name='customer_purchase_data', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Last Purchase Date per Customer\n",
    "query_last_purchase = '''\n",
    "CREATE VOLATILE TABLE last_purchase AS\n",
    "(\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        MAX(ts) AS last_purchase_ts\n",
    "    FROM customer_purchase_data\n",
    "    GROUP BY customer_id\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "'''\n",
    "execute_sql(query_last_purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quey_purchase_events= '''\n",
    "CREATE VOLATILE TABLE purchase_events AS\n",
    "(\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        ts,\n",
    "        product_id AS last_product,\n",
    "        LEAD(product_id) OVER (PARTITION BY customer_id ORDER BY ts) AS next_product\n",
    "    FROM customer_purchase_data\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "'''\n",
    "execute_sql(quey_purchase_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Global Max Timestamp\n",
    "query_max_ts = '''\n",
    "CREATE VOLATILE TABLE max_ts_table AS\n",
    "(\n",
    "    SELECT MAX(ts) AS max_ts\n",
    "    FROM customer_purchase_data\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "'''\n",
    "execute_sql(query_max_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d45139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Recency\n",
    "query_recency = '''\n",
    "CREATE VOLATILE TABLE recency_per_purchase AS\n",
    "(\n",
    "    SELECT\n",
    "        pe.customer_id,\n",
    "        pe.ts,\n",
    "        CASE \n",
    "            WHEN LAG(pe.ts) OVER (PARTITION BY pe.customer_id ORDER BY pe.ts) IS NULL THEN NULL\n",
    "            ELSE CAST(pe.ts AS DATE) - CAST(LAG(pe.ts) OVER (PARTITION BY pe.customer_id ORDER BY pe.ts) AS DATE)\n",
    "        END AS recency_days\n",
    "    FROM purchase_events pe\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "'''\n",
    "execute_sql(query_recency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Frequency (6 months)\n",
    "query_frequency = '''\n",
    "CREATE VOLATILE TABLE frequency_per_purchase AS\n",
    "(\n",
    "    SELECT\n",
    "        pe.customer_id,\n",
    "        pe.ts,\n",
    "        COUNT(*) AS frequency_6m\n",
    "    FROM purchase_events pe\n",
    "    JOIN customer_purchase_data p\n",
    "      ON pe.customer_id = p.customer_id\n",
    "     AND CAST(p.ts AS DATE) BETWEEN ADD_MONTHS(CAST(pe.ts AS DATE), -6) AND CAST(pe.ts AS DATE)\n",
    "    GROUP BY pe.customer_id, pe.ts\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "'''\n",
    "execute_sql(query_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39023981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Monetary Value (6 months)\n",
    "query_monetary = '''\n",
    "CREATE VOLATILE TABLE monetary_per_purchase AS\n",
    "(\n",
    "    SELECT\n",
    "        pe.customer_id,\n",
    "        pe.ts,\n",
    "        AVG(p.price) AS avg_price_6m,\n",
    "        SUM(p.price) AS total_price_6m\n",
    "    FROM purchase_events pe\n",
    "    JOIN customer_purchase_data p\n",
    "      ON pe.customer_id = p.customer_id\n",
    "     AND CAST(p.ts AS DATE) BETWEEN ADD_MONTHS(CAST(pe.ts AS DATE), -6) AND CAST(pe.ts AS DATE)\n",
    "    GROUP BY pe.customer_id, pe.ts\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "\n",
    "'''\n",
    "execute_sql(query_monetary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17563e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Average Days Between Purchases\n",
    "query_avg_days_between = '''\n",
    "CREATE VOLATILE TABLE avg_days_between_per_purchase AS\n",
    "(\n",
    "    SELECT\n",
    "        pe.customer_id,\n",
    "        pe.ts,\n",
    "        AVG(CAST(p2.ts AS DATE) - CAST(p1.ts AS DATE)) AS avg_days_between_purchase\n",
    "    FROM purchase_events pe\n",
    "    JOIN customer_purchase_data p1\n",
    "      ON pe.customer_id = p1.customer_id AND p1.ts <= pe.ts\n",
    "    JOIN customer_purchase_data p2\n",
    "      ON pe.customer_id = p2.customer_id AND p2.ts <= pe.ts\n",
    "         AND p2.ts > p1.ts\n",
    "    GROUP BY pe.customer_id, pe.ts\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "\n",
    "'''\n",
    "execute_sql(query_avg_days_between)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Product Diversity\n",
    "query_product_diversity = '''\n",
    "CREATE VOLATILE TABLE product_diversity_per_purchase AS\n",
    "(\n",
    "    SELECT\n",
    "        pe.customer_id,\n",
    "        pe.ts,\n",
    "        COUNT(DISTINCT p.product_id) AS product_diversity\n",
    "    FROM purchase_events pe\n",
    "    JOIN customer_purchase_data p\n",
    "      ON pe.customer_id = p.customer_id AND p.ts <= pe.ts\n",
    "    GROUP BY pe.customer_id, pe.ts\n",
    ") WITH DATA ON COMMIT PRESERVE ROWS;\n",
    "\n",
    "'''\n",
    "execute_sql(query_product_diversity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa87ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Final Feature Table\n",
    "query_final_features = '''\n",
    "CREATE MULTISET TABLE final_features AS\n",
    "(\n",
    "    SELECT \n",
    "        pe.customer_id,\n",
    "        pe.ts,\n",
    "        r.recency_days,\n",
    "        f.frequency_6m,\n",
    "        m.avg_price_6m,\n",
    "        m.total_price_6m,\n",
    "        adb.avg_days_between_purchase,\n",
    "        pe.last_product,\n",
    "        pd.product_diversity,\n",
    "        pe.next_product\n",
    "    FROM purchase_events pe\n",
    "    LEFT JOIN recency_per_purchase r ON pe.customer_id = r.customer_id AND pe.ts = r.ts\n",
    "    LEFT JOIN frequency_per_purchase f ON pe.customer_id = f.customer_id AND pe.ts = f.ts\n",
    "    LEFT JOIN monetary_per_purchase m ON pe.customer_id = m.customer_id AND pe.ts = m.ts\n",
    "    LEFT JOIN avg_days_between_per_purchase adb ON pe.customer_id = adb.customer_id AND pe.ts = adb.ts\n",
    "    LEFT JOIN product_diversity_per_purchase pd ON pe.customer_id = pd.customer_id AND pe.ts = pd.ts\n",
    "    WHERE pe.next_product IS NOT NULL  -- Only rows with a known next product\n",
    ") WITH DATA NO PRIMARY INDEX;\n",
    "'''\n",
    "execute_sql(query_final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea04d73",
   "metadata": {},
   "source": [
    "### Load Final Features Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f397e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_final_features=DataFrame(\"final_features\")\n",
    "tdf_final_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3c163",
   "metadata": {},
   "source": [
    "### Check data types and basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa971da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\", tdf_final_features.shape)\n",
    "print(\"\\nColumn information:\")\n",
    "tdf_final_features.info()\n",
    "print(\"\\nBasic statistics:\")\n",
    "tdf_final_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6989cf",
   "metadata": {},
   "source": [
    "### Checking for Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsum = ColumnSummary(\n",
    "    data  = tdf_final_features,\n",
    "    target_columns = [':']\n",
    ")\n",
    "colsum.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9bbeb",
   "metadata": {},
   "source": [
    "### Handle Missing Values with Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit imputer\n",
    "impute_fit_output = SimpleImputeFit(\n",
    "    data=tdf_final_features,\n",
    "    literals_columns=\"avg_days_between_purchase\",\n",
    "    literals=\"0\"\n",
    ")\n",
    "\n",
    "# Step 2: Apply transformation\n",
    "tdf_final_features_clean = SimpleImputeTransform(\n",
    "    data=tdf_final_features,\n",
    "    object=impute_fit_output.output\n",
    ").result   # This is now a teradataml DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsum = ColumnSummary(\n",
    "    data  = tdf_final_features_clean,\n",
    "    target_columns = [':']\n",
    ")\n",
    "colsum.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed6f2c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85acd4",
   "metadata": {},
   "source": [
    "### Encode Categorical Features (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fit object to encode \"last_product\" column\n",
    "hot_fit = OneHotEncodingFit(\n",
    "    data=tdf_final_features_clean,\n",
    "    is_input_dense=True,\n",
    "    target_column=['last_product'],\n",
    "    category_counts=[37],  # Must be a list\n",
    "    approach=\"auto\",\n",
    "    other_column=\"other\"\n",
    ")\n",
    "\n",
    "# Print the result DataFrame\n",
    "hot_fit.result\n",
    "# Apply the one-hot encoding using the fit object\n",
    "hot_encoded = OneHotEncodingTransform(\n",
    "    data=tdf_final_features_clean,\n",
    "    object=hot_fit,          # Use the fit object\n",
    "    is_input_dense=True      # Required parameter\n",
    ")\n",
    "\n",
    "# Show the transformed dataset\n",
    "hot_encoded.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb3fb",
   "metadata": {},
   "source": [
    "### Encode Target Column (Ordinal Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ordinal encoding fit object for next_product\n",
    "ordinal_fit = OrdinalEncodingFit(\n",
    "    target_column=['next_product'],   # Column to encode\n",
    "    data=tdf_final_features_clean,\n",
    "    default_value=-1                  # For unknown/unseen categories\n",
    ")\n",
    "\n",
    "# Show the mapping result\n",
    "ordinal_fit.result\n",
    "ordinal_encoded = OrdinalEncodingTransform(\n",
    "    data=tdf_final_features_clean,\n",
    "    object=ordinal_fit\n",
    ")\n",
    "\n",
    "# View transformed dataset\n",
    "ordinal_encoded.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853ae50",
   "metadata": {},
   "source": [
    "### Scale Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling on numeric columns (example: recency_days)\n",
    "scale_fit = ScaleFit(\n",
    "    data=tdf_final_features_clean,\n",
    "    target_columns=[\n",
    "        \"recency_days\", \n",
    "        \"frequency_6m\", \n",
    "        \"avg_price_6m\", \n",
    "        \"total_price_6m\", \n",
    "        \"avg_days_between_purchase\",\n",
    "        \"product_diversity\"\n",
    "    ],\n",
    "    scale_method=\"RANGE\",   # Scale to [0,1] range\n",
    "    miss_value=\"KEEP\",      # Keep missing values\n",
    "    global_scale=False      # Scale each column independently\n",
    ")\n",
    "\n",
    "# Show the scaling output (min/max, etc.)\n",
    "scale_fit.output\n",
    "scale_transformed = ScaleTransform(\n",
    "    data=tdf_final_features_clean,\n",
    "    object=scale_fit\n",
    ")\n",
    "\n",
    "# View transformed scaled data\n",
    "scale_transformed.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437014b7",
   "metadata": {},
   "source": [
    "## Feature Combination and Training/Testing Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine scaling, one-hot, and ordinal encoding\n",
    "combined_transform = ColumnTransformer(\n",
    "    input_data=tdf_final_features_clean,\n",
    "    scale_fit_data=scale_fit.output,           # Use .output here\n",
    "    onehotencoding_fit_data=hot_fit.result,    # Use .result for OneHotEncodingFit\n",
    "    ordinalencoding_fit_data=ordinal_fit.result  # Use .result for OrdinalEncodingFit\n",
    ")\n",
    "\n",
    "# Final transformed DataFrame\n",
    "tdf_transformed = combined_transform.result\n",
    "tdf_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in tdf_transformed.columns if c != 'next_product'] + ['next_product']\n",
    "tdf_transformed = tdf_transformed[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop columns not needed for modeling\n",
    "tdf_dataset = tdf_transformed.drop(columns=['customer_id', 'ts', 'last_product', 'last_product_other'])\n",
    "\n",
    "# 2. Split dataset into train and test\n",
    "tdf_sample = tdf_dataset.sample(frac=[0.8, 0.2])\n",
    "df_train = tdf_sample[tdf_sample[\"sampleid\"] == 1].drop(\"sampleid\", axis=1)\n",
    "df_test  = tdf_sample[tdf_sample[\"sampleid\"] == 2].drop(\"sampleid\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set = \" + str(df_train.shape[0]) + \". Testing Set = \" + str(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656227d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df_train, table_name = 'final_data_train', if_exists = 'replace')\n",
    "copy_to_sql(df_test, table_name = 'final_data_test', if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e91881",
   "metadata": {},
   "source": [
    "# ***Generate a predictive model using lightGBM and sklearn***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b7c30",
   "metadata": {},
   "source": [
    "# Model Training with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89997e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_train_df = df_train.to_pandas(all_rows = True)\n",
    "final_data_train_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into features (X_train) and target (y_train)\n",
    "y_train = final_data_train_df.iloc[:, -1]\n",
    "X_train = final_data_train_df.iloc[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightGBM as a Scikit-learn classifier\n",
    "lgbm_model = LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=len(y_train.unique()),\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Create PMML pipeline and train\n",
    "pipeline = PMMLPipeline([\n",
    "    (\"classifier\", lgbm_model)\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Export to PMML\n",
    "sklearn2pmml(pipeline, \"lgbm_next_product.pmml\", with_repr=True)\n",
    "print(\"Model trained and saved as lgbm_next_product.pmml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea3b2e",
   "metadata": {},
   "source": [
    "# ***Import model in Vantage***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cbdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PMML model into Vantage\n",
    "try:\n",
    "    res = save_byom(\n",
    "        model_id='lgbm_next_product',       # Unique ID for your model\n",
    "        model_file='lgbm_next_product.pmml', # PMML file from sklearn2pmml\n",
    "        table_name='lgbm_models'            # Table to store the model\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    # If the model ID already exists, delete and overwrite\n",
    "    if str(e.args).find('TDML_2200') >= 1:\n",
    "        res = delete_byom(model_id='lgbm_next_product', table_name='lgbm_models')\n",
    "        res = save_byom(model_id='lgbm_next_product', model_file='lgbm_next_product.pmml', table_name='lgbm_models')\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd217289",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame('lgbm_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd84f2c",
   "metadata": {},
   "source": [
    "# ***Score the model in-database***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b41f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Retrieve the LightGBM model from Vantage\n",
    "model_tdf = retrieve_byom(\"lgbm_next_product\", table_name='lgbm_models')\n",
    "\n",
    "# 2. Run predictions on df_test\n",
    "result = PMMLPredict(\n",
    "    modeldata=model_tdf,       \n",
    "    newdata=df_test,           \n",
    "    accumulate=['recency_days', 'frequency_6m', 'avg_price_6m', \n",
    "                'total_price_6m', 'avg_days_between_purchase', \n",
    "                'product_diversity'],  \n",
    "    overwrite_cached_models='*'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b349b2f",
   "metadata": {},
   "source": [
    "## Extracting Predicted Class from Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.result.to_pandas(all_rows=True)\n",
    "\n",
    "# Find class with max probability\n",
    "def get_top_class(prob_json):\n",
    "    probs = json.loads(prob_json)\n",
    "    return max(probs, key=probs.get)  # e.g., \"probability(20)\"\n",
    "\n",
    "# Extract numeric part\n",
    "def extract_number(prob_str):\n",
    "    match = re.search(r'\\d+', prob_str)\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "df['predicted_class'] = df['json_report'].apply(get_top_class)\n",
    "df['predicted_class_num'] = df['predicted_class'].apply(extract_number)\n",
    "\n",
    "print(df[['predicted_class_num', 'json_report']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296469ea",
   "metadata": {},
   "source": [
    "# ***Cleanup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tables to drop\n",
    "tables_to_drop = [\n",
    "    \"npath_purchase_data\",\n",
    "    \"customer_purchase_data\",\n",
    "    \"last_purchase\",\n",
    "    \"purchase_events\",\n",
    "    \"max_ts_table\",\n",
    "    \"recency_per_purchase\",\n",
    "    \"frequency_per_purchase\",\n",
    "    \"monetary_per_purchase\",\n",
    "    \"avg_days_between_per_purchase\",\n",
    "    \"product_diversity_per_purchase\",\n",
    "    \"final_features\",\n",
    "    \"final_data_train\",\n",
    "    \"final_data_test\",\n",
    "    \"lgbm_models\"\n",
    "]\n",
    "\n",
    "# Drop each table if it exists\n",
    "for table in tables_to_drop:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "        print(f\"Table '{table}' dropped successfully.\")\n",
    "    except:\n",
    "        print(f\"Table '{table}' does not exist or could not be dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
